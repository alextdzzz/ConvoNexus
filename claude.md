# MeetingNexus: Real-Time Meeting Mind Map Generator

## Overview

A fully functional real-time system that transforms meeting conversations into interactive knowledge graphs as they happen. Built using RealtimeSTT for speech recognition, OpenAI for knowledge extraction, and vis.js for graph visualization, connected via WebSocket for <5 second end-to-end latency.

## âœ… Implemented Architecture

```
ğŸ¤ Audio Stream â†’ RealtimeSTT â†’ WebSocket â†’ Node.js Server â†’ OpenAI â†’ vis.js Graph
     â†“              â†“            â†“             â†“            â†“         â†“
Live Audio    Speech-to-Text  Real-time    Knowledge    AI Entity  Interactive
Capture       Processing      Updates      Processing   Extraction Visualization
```

## ğŸ§© Core Components (Built & Working)

### 1. Audio Service (`audio_service.py`)
**Role**: Real-time speech capture and transcription
- **Technology**: RealtimeSTT with Whisper models
- **Models**: `base.en` (accuracy) + `tiny.en` (real-time processing)
- **Features**: 
  - Speaker identification with manual switching
  - Confidence scoring and voice activity detection
  - WebSocket reconnection and error recovery
  - Duplicate prevention and text preprocessing

**Live Data Flow**:
```python
def on_final_text(self, text):
    # Only sends complete sentences, not partial updates
    data = {
        "type": "transcript",
        "speaker": "Speaker_1",
        "text": "Let's discuss the Q3 budget",
        "is_final": True,
        "confidence": 0.95,
        "timestamp": "2025-01-24T16:30:00Z"
    }
    await self.websocket.send(json.dumps(data))
```

### 2. WebSocket Server (`server.js`)
**Role**: Central coordinator and knowledge processor
- **Technology**: Node.js + Express + WebSocket + OpenAI GPT-3.5-turbo
- **Intelligence**: Batches transcripts (10 seconds OR 3 sentences) for context
- **Features**:
  - Transcript deduplication and participant tracking
  - Meeting state management (start/stop/clear)
  - Graph state persistence and client synchronization
  - Comprehensive error handling and logging

**Knowledge Extraction Process**:
```javascript
async extractMeetingKnowledge(transcripts) {
    const prompt = `Extract knowledge graph from:
    ${transcripts.map(t => `${t.speaker}: ${t.text}`).join('\n')}
    
    Focus on: people, topics, decisions, action items, deadlines
    Return JSON: [["Entity1", "relationship", "Entity2"], ["Entity1", "#color"]]
    
    Colors: People=#4CAF50, Topics=#2196F3, Decisions=#FF9800, 
            Actions=#F44336, Deadlines=#9C27B0`;
    
    return await openai.chat.completions.create({...});
}
```

### 3. Frontend Visualization (`public/app.js` + `index.html`)
**Role**: Interactive graph visualization and meeting controls
- **Technology**: Vanilla JavaScript + vis.js Network + WebSocket client
- **Layout**: Vertical design (graph top, transcript bottom) with zoom controls
- **Features**:
  - Real-time graph updates with animation
  - Zoom, pan, and fit controls with mouse/keyboard interaction
  - Meeting management (start/stop/clear/export)
  - Live transcript display with speaker identification
  - Graph state loading and incremental updates

**Graph Update Pipeline**:
```javascript
handleGraphUpdate(updates) {
    updates.forEach(update => {
        if (update.length === 3) {
            // Add relationship: ["Speaker_1", "discussed", "Q3 budget"]
            const [entity1, relation, entity2] = update;
            this.graphData.nodes.add({id: entity1, label: entity1});
            this.graphData.edges.add({from: entity1, to: entity2, label: relation});
        } else if (update.length === 2 && update[1].startsWith('#')) {
            // Set color: ["Q3 budget", "#2196F3"]
            this.graphData.nodes.update({id: update[0], color: update[1]});
        }
    });
}
```

## ğŸ“Š Complete Data Flow Example

**User speaks**: *"Let's discuss the Q3 budget. John should handle the marketing analysis by Friday."*

### Step 1: Speech Processing (3-5 seconds)
```
ğŸ¤ RealtimeSTT captures audio
â”œâ”€â”€ Real-time: "Let's..." â†’ "Let's discuss..." â†’ "Let's discuss the Q3..."
â”œâ”€â”€ Final sentence 1: "Let's discuss the Q3 budget."
â””â”€â”€ Final sentence 2: "John should handle the marketing analysis by Friday."

ğŸ“¡ WebSocket messages sent (only final sentences):
â”œâ”€â”€ {"type": "transcript", "text": "Let's discuss the Q3 budget.", "is_final": true}
â””â”€â”€ {"type": "transcript", "text": "John should handle...by Friday.", "is_final": true}
```

### Step 2: Knowledge Processing (2-3 seconds)
```
ğŸ¤– Server batches 2 sentences â†’ OpenAI GPT-3.5-turbo
â”œâ”€â”€ Input: "Speaker_1: Let's discuss the Q3 budget.\nSpeaker_1: John should handle..."
â”œâ”€â”€ AI extracts entities: ["Speaker_1", "Q3 budget", "John", "marketing analysis", "Friday"]
â”œâ”€â”€ AI identifies relationships: discussed, assigned, due
â””â”€â”€ Output: [["Speaker_1","discussed","Q3 budget"], ["John","assigned","marketing analysis"]...]

ğŸ“¡ WebSocket broadcast: {"type": "graph_update", "data": [...]}
```

### Step 3: Graph Visualization (0.5 seconds)
```
ğŸ¯ vis.js renders updated graph:

[Speaker_1]â”€â”€â”€â”€discussedâ”€â”€â”€â”€â–¶[Q3 budget]
   (green)                     (blue)
      
[John]â”€â”€â”€â”€assignedâ”€â”€â”€â”€â–¶[marketing analysis]â”€â”€â”€â”€dueâ”€â”€â”€â”€â–¶[Friday]
(green)                  (red - Action)              (purple - Deadline)

ğŸ•’ Total latency: ~5 seconds from speech to visual graph
```

## ğŸ› ï¸ Implemented Features

**âœ… Real-Time Processing**:
- Speech-to-text with <3 second transcription latency
- AI knowledge extraction with contextual batching
- Live graph updates with smooth animations
- WebSocket communication with auto-reconnection

**âœ… Interactive Visualization**:
- Zoom, pan, and fit controls for graph navigation
- Color-coded entities (people=green, topics=blue, etc.)
- Drag nodes, click interactions, and responsive layout
- Fixed-size container prevents infinite scroll

**âœ… Meeting Management**:
- Start/stop meeting controls with audio service sync
- Speaker switching (manual: press 'n' or click button)
- Export capabilities (JSON, transcript, meeting summary)
- Clear graph functionality and session persistence

**âœ… Error Resilience**:
- WebSocket reconnection on connection drops
- Duplicate transcript prevention (client & server side)
- Graceful degradation if components fail independently
- Comprehensive logging and debugging capabilities

## ğŸ“ Final File Structure
```
ConvoNexus/
â”œâ”€â”€ audio_service.py          # Python: RealtimeSTT speech processor
â”œâ”€â”€ server.js                 # Node.js: WebSocket server + OpenAI
â”œâ”€â”€ start.js                  # Unified launcher (all components)
â”œâ”€â”€ package.json              # Node dependencies + launch scripts
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ .env                      # Environment variables (API keys)
â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ index.html           # UI layout with zoom controls
â”‚   â””â”€â”€ app.js               # Graph visualization + WebSocket client
â”œâ”€â”€ GraphGPT/                # Reference implementation
â”œâ”€â”€ RealtimeSTT_source/      # Reference source code
â””â”€â”€ README.md               # Complete setup instructions
```

## ğŸ¯ Performance Metrics (Achieved)

**âœ… User Experience**:
- Concepts appear within 5 seconds of being mentioned
- Smooth graph animations with interactive controls
- Intuitive meeting controls and speaker management
- Responsive design for desktop/tablet/mobile

**âœ… Technical Performance**:
- <5 second end-to-end latency (speech â†’ graph)
- Stable WebSocket connections with auto-recovery
- Handles 1-hour meetings without memory issues
- Efficient batching prevents API rate limits

## ğŸš€ Usage Instructions

**Single Command Startup**:
```bash
# Windows: start.bat
# Linux/Mac: ./start.sh
# Direct: node start.js
```

**Meeting Flow**:
1. System auto-starts all components
2. Browser opens to http://localhost:3001
3. Click "Start Meeting" 
4. Begin speaking â†’ Watch live graph generation
5. Use 'n' to switch speakers, zoom controls to navigate
6. Export data when complete

## ğŸ”§ Configuration Options

**Audio Models**: Edit `audio_service.py` for different Whisper models
**OpenAI Settings**: Configure model/temperature in `.env`
**Graph Appearance**: Modify colors and layout in `app.js`
**Processing Intervals**: Adjust batching triggers in `server.js`

## ğŸ¨ Customization Examples

**Custom Entity Types**:
```javascript
// Add new categories in server.js OpenAI prompt
- Risks: #FF5722 (deep orange)
- Resources: #795548 (brown)
- Milestones: #00BCD4 (cyan)
```

**Meeting Types**: Specialized prompts for sprint planning, client presentations, technical reviews, etc.

## ğŸ“ˆ Future Enhancements

**Next Phase**: Multi-user collaboration, persistent storage, calendar integration
**Advanced**: Multi-language support, custom vocabularies, mobile apps
**Enterprise**: Zoom/Teams integration, advanced analytics, compliance features

This system demonstrates a complete real-time AI-powered meeting analysis tool built in 1-2 days using modern open-source components, providing immediate value with a clear path for enhancement and scaling.