# MeetingNexus ğŸ¯

**Real-time meeting transcription with live knowledge graph visualization**

Transform meeting conversations into interactive mind maps as they happen. Built by combining GraphGPT's knowledge extraction with RealtimeSTT's speech-to-text capabilities.

## ğŸš€ Quick Start (5 minutes)

### Prerequisites
- **Python 3.8+** (for audio processing)
- **Node.js 16+** (for server and frontend)
- **OpenAI API key** (for knowledge extraction)
- **Microphone access**

### 1. Clone & Install Dependencies

```bash
# Navigate to your MeetingNexus directory
cd ConvoNexus

# Install Node.js dependencies
npm install

# Install Python dependencies
pip install -r requirements.txt
```

### 2. Configure Environment

```bash
# Copy environment template
cp .env.example .env

# Edit .env and add your OpenAI API key
# OPENAI_API_KEY=your_api_key_here
```

### 3. Start the System

**Terminal 1: Start the WebSocket Server**
```bash
node server.js
```

**Terminal 2: Start the Audio Service**
```bash
python audio_service.py
```

**Terminal 3: Open the Web Interface**
```bash
# Open http://localhost:3001 in your browser
# Or if you have a local server:
# python -m http.server 8000 -d public
```

### 4. Begin Recording
1. Click **"Start Meeting"** in the web interface
2. In the audio service terminal, type **`s`** to start recording
3. Start speaking - watch the knowledge graph build in real-time!

---

## ğŸ“‹ System Architecture

```
Audio Stream â†’ RealtimeSTT â†’ WebSocket â†’ Server â†’ OpenAI â†’ Live Knowledge Graph
     â†“              â†“            â†“         â†“        â†“              â†“
Live Audio    Speech-to-Text  Real-time  Node.js  Knowledge    Interactive
Capture       Processing      Updates    Server   Extraction   Visualization
```

## ğŸ› ï¸ Component Details

### Audio Service (`audio_service.py`)
- **Purpose**: Real-time speech-to-text using RealtimeSTT
- **Features**: Speaker detection, confidence scoring, WebSocket output
- **Models**: Whisper base.en (accuracy) + tiny.en (real-time)
- **Controls**: 's' = start, 'q' = quit, 'n' = next speaker

### WebSocket Server (`server.js`)
- **Purpose**: Process transcripts and coordinate knowledge extraction
- **Features**: OpenAI integration, graph state management, multi-client support
- **Endpoints**: Meeting control, status monitoring, graph export
- **Processing**: Batches transcripts every 10 seconds or 3 sentences

### Web Interface (`public/`)
- **Purpose**: Real-time visualization using vis.js network graphs
- **Features**: Live transcript, interactive graph, meeting controls
- **Colors**: People (green), Topics (blue), Decisions (orange), Actions (red), Deadlines (purple)
- **Export**: JSON graph data, plain text transcripts, meeting summaries

---

## ğŸ¯ Usage Guide

### Starting a Meeting
1. **Launch components** in order: Server â†’ Audio Service â†’ Browser
2. **Start meeting** via web interface
3. **Begin recording** with audio service
4. **Watch live updates** as concepts appear in the graph

### During the Meeting
- **Graph builds automatically** from conversation
- **Switch speakers** by typing `n` in audio service
- **View live transcript** in right panel
- **Interact with graph** - drag, zoom, click nodes

### Ending the Meeting
1. **Stop recording** with `q` in audio service
2. **Stop meeting** via web interface
3. **Export data** - graph, transcript, or summary
4. **Clear graph** for next meeting (optional)

---

## ğŸ“Š Features

### Real-Time Processing
- **<5 second latency** from speech to graph
- **Continuous updates** as meeting progresses
- **Speaker identification** with automatic switching
- **Partial transcripts** show current speech

### Knowledge Extraction
- **Entities**: People, topics, decisions, action items
- **Relationships**: Automatically detected connections
- **Color coding**: Visual distinction by entity type
- **Meeting context**: Specialized prompts for business discussions

### Interactive Visualization
- **Drag and zoom** graph exploration
- **Node highlighting** shows connections
- **Real-time animations** for new additions
- **Export capabilities** for sharing and analysis

### Meeting Management
- **Multi-participant** support
- **Session persistence** during connection issues
- **Clear and restart** functionality
- **Status monitoring** for all components

---

## ğŸ”§ Configuration Options

### Audio Models (edit `audio_service.py`)
```python
'model': 'base.en',         # Main model (tiny, base, small, medium, large)
'realtime_model_type': 'tiny.en',  # Real-time model for partial updates
'language': 'en',           # Language code
```

### Knowledge Extraction (edit `server.js`)
```javascript
OPENAI_MODEL=gpt-3.5-turbo  # Use gpt-4 for better extraction
OPENAI_TEMPERATURE=0.3      # Lower = more consistent
OPENAI_MAX_TOKENS=800       # Longer responses
```

### Performance Tuning
```python
# Audio sensitivity (audio_service.py)
'silero_sensitivity': 0.1,   # Voice activity detection
'post_speech_silence_duration': 0.7,  # End of speech detection

# Processing intervals (server.js)
KNOWLEDGE_EXTRACTION_INTERVAL=10000  # Process every 10 seconds
MAX_TRANSCRIPT_BUFFER_SIZE=5         # Or every 5 sentences
```

---

## ğŸš¨ Troubleshooting

### Audio Issues
```bash
# Check microphone permissions
# Windows: Settings â†’ Privacy â†’ Microphone
# Mac: System Preferences â†’ Security â†’ Microphone

# Test audio input
python -c "import pyaudio; print('PyAudio working')"

# Try different audio device
# Edit audio_service.py: 'input_device_index': 1
```

### Connection Issues
```bash
# Verify WebSocket server is running
netstat -an | grep 8765

# Check if ports are available
netstat -an | grep 3001

# Test WebSocket connection
# Browser console: new WebSocket('ws://localhost:8765')
```

### OpenAI Issues
```bash
# Verify API key is valid
curl -H "Authorization: Bearer YOUR_API_KEY" https://api.openai.com/v1/models

# Check .env file is loaded
node -e "require('dotenv').config(); console.log(process.env.OPENAI_API_KEY)"
```

### Performance Issues
```bash
# For GPU acceleration (optional)
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118

# For lower resource usage
# Use smaller models: tiny.en, base.en
# Increase processing intervals
# Reduce WebSocket update frequency
```

---

## ğŸ“ File Structure

```
ConvoNexus/
â”œâ”€â”€ audio_service.py       # Python audio processing service
â”œâ”€â”€ server.js             # Node.js WebSocket server
â”œâ”€â”€ package.json          # Node.js dependencies
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ .env.example         # Environment variables template
â”œâ”€â”€ README.md           # This file
â”œâ”€â”€ public/             # Web interface
â”‚   â”œâ”€â”€ index.html     # Main UI
â”‚   â””â”€â”€ app.js         # Frontend JavaScript
â”œâ”€â”€ GraphGPT/          # Original GraphGPT (reference)
â””â”€â”€ RealtimeSTT/       # Original RealtimeSTT (reference)
```

---

## ğŸ¨ Customization

### Adding Custom Entity Types
Edit the OpenAI prompt in `server.js`:
```javascript
// Add new entity types and colors
- Projects: #00BCD4 (cyan)
- Risks: #FF5722 (deep orange)
- Resources: #795548 (brown)
```

### Modifying Graph Appearance
Edit visualization options in `public/app.js`:
```javascript
nodes: {
    shape: 'circle',    // dot, circle, database, box, etc.
    size: 25,          // Default node size
    font: { size: 16 } // Label font size
}
```

### Custom Meeting Types
Create specialized prompts for different meeting types:
- Sprint planning meetings
- Client presentations  
- Technical design reviews
- Board meetings

---

## ğŸš€ Deployment

### Local Network Access
```bash
# Allow external connections (server.js)
this.app.listen(this.port, '0.0.0.0', () => {
    console.log(`Server running on http://0.0.0.0:${this.port}`);
});

# Access via: http://YOUR_IP:3001
```

### Cloud Deployment
- **Server**: Deploy Node.js server to Heroku, AWS, or DigitalOcean
- **Audio Service**: Run locally (requires microphone access)
- **Database**: Add MongoDB/PostgreSQL for meeting persistence

---

## ğŸ¤ Contributing

Built on open-source foundations:
- **GraphGPT**: https://github.com/varunshenoy/GraphGPT
- **RealtimeSTT**: https://github.com/KoljaB/RealtimeSTT

### Extending the System
- Add persistent meeting storage
- Implement user authentication
- Create mobile app companion
- Add integration with Zoom/Teams/Meet
- Build advanced analytics dashboard

---

## ğŸ“„ License

This project combines multiple open-source components. Please respect the licenses of:
- GraphGPT (MIT License)
- RealtimeSTT (MIT License)
- OpenAI API (Commercial Terms)

---

## ğŸ¯ Success Metrics

**Working System Indicators:**
- âœ… Audio service connects to WebSocket server
- âœ… Speech appears in transcript within 3 seconds
- âœ… Knowledge graph updates within 15 seconds of key concepts
- âœ… Graph visualization is interactive and responsive
- âœ… Meeting data can be exported successfully

**Expected Performance:**
- **Latency**: <5 seconds end-to-end
- **Accuracy**: 90%+ for clear English speech
- **Uptime**: No crashes during 1-hour meetings
- **Resource Usage**: <2GB RAM, moderate CPU usage

---

## ğŸ“ Quick Support

### Most Common Issues
1. **No audio detected**: Check microphone permissions and device selection
2. **WebSocket connection failed**: Ensure server is running on correct port
3. **No graph updates**: Verify OpenAI API key and check server logs
4. **High latency**: Try smaller audio models or adjust processing intervals

### Quick Tests
```bash
# Test audio input
python -c "from RealtimeSTT import AudioToTextRecorder; print('Audio OK')"

# Test WebSocket server
curl http://localhost:3001/api/meeting/status

# Test OpenAI connection
node -e "const OpenAI = require('openai'); console.log('OpenAI module loaded')"
```

---

**Ready to transform your meetings? ğŸš€**

Run the three commands above and start building knowledge graphs from your conversations in real-time!